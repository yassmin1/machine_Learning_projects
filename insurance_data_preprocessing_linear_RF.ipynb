{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading libraries and functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set()\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "plt.rcParams.update({'font.family' : 'normal',\n",
    "        'font.weight' : 'bold','font.size': 22})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot a boxplot and a histogram along the same scale.\n",
    "def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):\n",
    "    \"\"\"\n",
    "    Boxplot and histogram combined\n",
    "\n",
    "    data: dataframe\n",
    "    feature: dataframe column\n",
    "    figsize: size of figure (default (12,7))\n",
    "    kde: whether to the show density curve (default False)\n",
    "    bins: number of bins for histogram (default None)\n",
    "    \"\"\"\n",
    "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
    "        nrows=2,  # Number of rows of the subplot grid= 2\n",
    "        sharex=True,  # x-axis will be shared among all subplots\n",
    "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
    "        figsize=figsize,\n",
    "    )  # creating the 2 subplots\n",
    "    sns.boxplot(\n",
    "        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n",
    "    )  # boxplot will be created and a star will indicate the mean value of the column\n",
    "    sns.histplot(\n",
    "        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette=\"winter\"\n",
    "    ) if bins else sns.histplot(\n",
    "        data=data, x=feature, kde=kde, ax=ax_hist2\n",
    "    )  # For histogram\n",
    "    ax_hist2.axvline(\n",
    "        data[feature].mean(), color=\"green\", linestyle=\"--\"\n",
    "    )  # Add mean to the histogram\n",
    "    ax_hist2.axvline(\n",
    "        data[feature].median(), color=\"black\", linestyle=\"-\"\n",
    "    )  # Add median to the histogram\n",
    "\n",
    "# function to create labeled barplots\n",
    "\n",
    "\n",
    "\n",
    "def labeled_barplot(data, feature, perc=False, n=None):\n",
    "    \"\"\"\n",
    "    Barplot with percentage at the top\n",
    "\n",
    "    data: dataframe\n",
    "    feature: dataframe column\n",
    "    perc: whether to display percentages instead of count (default is False)\n",
    "    n: displays the top n category levels (default is None, i.e., display all levels)\n",
    "    \"\"\"\n",
    "\n",
    "    total = len(data[feature])  # length of the column\n",
    "    count = data[feature].nunique()\n",
    "    if n is None:\n",
    "        plt.figure(figsize=(count + 1, 5))\n",
    "    else:\n",
    "        plt.figure(figsize=(n + 1, 5))\n",
    "\n",
    "    plt.xticks(rotation=90, fontsize=15)\n",
    "    ax = sns.countplot(\n",
    "        data=data,\n",
    "        x=feature,\n",
    "        palette=\"Paired\",\n",
    "        order=data[feature].value_counts().index[:n].sort_values(),\n",
    "    )\n",
    "\n",
    "    for p in ax.patches:\n",
    "        if perc == True:\n",
    "            label = \"{:.1f}%\".format(\n",
    "                100 * p.get_height() / total\n",
    "            )  # percentage of each class of the category\n",
    "        else:\n",
    "            label = p.get_height()  # count of each level of the category\n",
    "\n",
    "        x = p.get_x() + p.get_width() / 2  # width of the plot\n",
    "        y = p.get_height()  # height of the plot\n",
    "\n",
    "        ax.annotate(\n",
    "            label,\n",
    "            (x, y),\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            size=12,\n",
    "            xytext=(0, 5),\n",
    "            textcoords=\"offset points\",\n",
    "        )  # annotate the percentage\n",
    "\n",
    "    plt.show()  # show the plot\n",
    "    \n",
    "def treat_outliers(df, col,whisk=1.5):\n",
    "    \n",
    "    \"\"\"\n",
    "    Treats outliers in a variable\n",
    "\n",
    "    df: dataframe\n",
    "    col: dataframe column\n",
    "    whisk:stader=1.5 but you can change it\n",
    "    \"\"\"\n",
    "    Q1 = df[col].quantile(0.25)  # 25th quantile\n",
    "    Q3 = df[col].quantile(0.75)  # 75th quantile\n",
    "    IQR = Q3 - Q1\n",
    "    Lower_Whisker = Q1 - whisk * IQR\n",
    "    Upper_Whisker = Q3 + whisk * IQR\n",
    "\n",
    "    # all the values smaller than Lower_Whisker will be assigned the value of Lower_Whisker\n",
    "    # all the values greater than Upper_Whisker will be assigned the value of Upper_Whisker\n",
    "    df[col] = np.clip(df[col], Lower_Whisker, Upper_Whisker)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def treat_outliers_all(df, col_list,whisk=1.5):\n",
    "    \"\"\"\n",
    "    Treat outliers in a list of variables\n",
    "\n",
    "    df: dataframe\n",
    "    col_list: list of dataframe columns\n",
    "    \"\"\"\n",
    "    for c in col_list:\n",
    "        df = treat_outliers(df, c,whisk=1.5)\n",
    "\n",
    "    return df\n",
    "# function to compute adjusted R-squared\n",
    "def adj_r2_score(predictors, targets, predictions):\n",
    "    r2 = r2_score(targets, predictions)\n",
    "    n = predictors.shape[0]\n",
    "    k = predictors.shape[1]\n",
    "    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n",
    "\n",
    "\n",
    "# function to compute MAPE\n",
    "def mape_score(targets, predictions):\n",
    "    return np.mean(np.abs(targets - predictions) / targets) * 100\n",
    "\n",
    "\n",
    "# function to compute different metrics to check performance of a regression model\n",
    "def model_performance_regression(model, predictors, target):\n",
    "    \"\"\"\n",
    "    Function to compute different metrics to check regression model performance\n",
    "\n",
    "    model: regressor\n",
    "    predictors: independent variables\n",
    "    target: dependent variable\n",
    "    \"\"\"\n",
    "\n",
    "    # predicting using the independent variables\n",
    "    pred = model.predict(predictors)\n",
    "\n",
    "    r2 = r2_score(target, pred)  # to compute R-squared\n",
    "    adjr2 = adj_r2_score(predictors, target, pred)  # to compute adjusted R-squared\n",
    "    rmse = np.sqrt(mean_squared_error(target, pred))  # to compute RMSE\n",
    "    mae = mean_absolute_error(target, pred)  # to compute MAE\n",
    "    mape = mape_score(target, pred)  # to compute MAPE\n",
    "\n",
    "    # creating a dataframe of metrics\n",
    "    df_perf = pd.DataFrame(\n",
    "        {\n",
    "            \"RMSE\": rmse,\n",
    "            \"MAE\": mae,\n",
    "            \"R-squared\": r2,\n",
    "            \"Adj. R-squared\": adjr2,\n",
    "            \"MAPE\": mape,\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "\n",
    "    return df_perf\n",
    "def treating_multicollinearity(predictors, target, high_vif_columns):\n",
    "    \"\"\"\n",
    "    Checking the effect of dropping the columns showing high multicollinearity\n",
    "    on model performance (adj. R-squared and RMSE)\n",
    "\n",
    "    predictors: independent variables\n",
    "    target: dependent variable\n",
    "    high_vif_columns: columns having high VIF\n",
    "    \"\"\"\n",
    "    # empty lists to store adj. R-squared and RMSE values\n",
    "    adj_r2 = []\n",
    "    rmse = []\n",
    "\n",
    "    # build ols models by dropping one of the high VIF columns at a time\n",
    "    # store the adjusted R-squared and RMSE in the lists defined previously\n",
    "    for cols in high_vif_columns:\n",
    "        # defining the new train set\n",
    "        train = predictors.loc[:, ~predictors.columns.str.startswith(cols)]\n",
    "\n",
    "        # create the model\n",
    "        olsmodel = sm.OLS(target, train).fit()\n",
    "\n",
    "        # adding adj. R-squared and RMSE to the lists\n",
    "        adj_r2.append(olsmodel.rsquared_adj)\n",
    "        rmse.append(np.sqrt(olsmodel.mse_resid))\n",
    "\n",
    "    # creating a dataframe for the results\n",
    "    temp = pd.DataFrame(\n",
    "        {\n",
    "            \"col\": high_vif_columns,\n",
    "            \"Adj. R-squared after_dropping col\": adj_r2,\n",
    "            \"RMSE after dropping col\": rmse,\n",
    "        }\n",
    "    ).sort_values(by=\"Adj. R-squared after_dropping col\", ascending=False)\n",
    "    temp.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return temp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading and exploring the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"insurance.csv\")\n",
    "print(f'There are {df.shape[0]} rows and {df.shape[1]} columns.')  \n",
    "np.random.seed(1)\n",
    "df.sample(n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()  # down to 82 columns after the initial 88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at which columns have the most missing values\n",
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all').T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot(df,'age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot(df,'bmi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_boxplot(df,'charges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_barplot(df,'smoker',perc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_barplot(df,'region',perc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_barplot(df,'children',perc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=df.select_dtypes('number'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(by='region').median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=df,y='charges',x='region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=df,y='bmi',x='children')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=df,y='bmi',x='region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(),cmap='Spectral_r',annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at value counts for non-numeric features\n",
    "\n",
    "num_to_display = 10  # defining this up here so it's easy to change later if I want\n",
    "for colname in df.dtypes[df.dtypes == 'object'].index:\n",
    "    val_counts = df[colname].value_counts(dropna=False)  # i want to see NA counts\n",
    "    print(val_counts[:num_to_display])\n",
    "    if len(val_counts) > num_to_display:\n",
    "        print(f'Only displaying first {num_to_display} of {len(val_counts)} values.')\n",
    "    print('\\n\\n') # just for more space between "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import f_oneway,ttest_ind,chi2_contingency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df,y='charges',x='children')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch0=df[df['children']==0]['charges']\n",
    "ch1=df[df['children']==1]['charges']\n",
    "ch2=df[df['children']==2]['charges'] \n",
    "ch3=df[df['children']==3]['charges']\n",
    "ch4=df[df['children']==4]['charges']\n",
    "ch5=df[df['children']==5]['charges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats,pvalue=f_oneway(ch0,ch1,ch2,ch3,ch4,ch5)\n",
    "print(\"tstat = \", stats, \", p-value = \", pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since p-value(0.003) < 0.05 (alpha), we reject the null hypothesis and conclude that atleast one of the mean charges in the six children category is unequal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "female=df[df['sex']=='female']['charges']\n",
    "male=df[df['sex']=='male']['charges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t,  p_value = ttest_ind(male, female)\n",
    "print(\"tstat = \",t, \", p_value = \", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since p-value(0.055) > 0.05 (alpha), we failed to reject the null hypothesis and conclude that there is no a difference in the two ( male and female) charges means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross=pd.crosstab(df.sex,df.region)\n",
    "chi2, pval, dof, exp_freq=chi2_contingency(cross)\n",
    "pval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since p-value(0.9) > 0.05 (alpha), we failed to reject the null hypothesis and conclude that the two features(sex,region) are independent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross=pd.crosstab(df.smoker,df.region)\n",
    "chi2, pval, dof, exp_freq=chi2_contingency(cross)\n",
    "pval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since p-value(0.06) > 0.05 (alpha), we failed to reject the null hypothesis and conclude that the two features(smoker,region) are independent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross=pd.crosstab(df.smoker,df.sex)\n",
    "chi2, pval, dof, exp_freq=chi2_contingency(cross)\n",
    "pval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since p-value(0.006) < 0.05 (alpha), we reject the null hypothesis and conclude that the two features(smoker,sex) are not independent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_no=df[df['smoker']=='no']['charges']\n",
    "s_yes=df[df['smoker']=='yes']['charges']\n",
    "t, p_value = ttest_ind(s_no, s_yes)\n",
    "print(\"tstat = \",t, \", p_value = \", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since p-value(0.000008) < 0.05 (alpha), we failed to reject the null hypothesis and conclude that the mean fof the two featurs are not equal. the smoker and non smoker has different charges "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_se=df[df['region']=='southeast']['charges']\n",
    "region_sw=df[df['region']=='southwest' ]['charges']   \n",
    "region_nw=df[df['region']=='northwest' ]['charges']   \n",
    "region_ne=df[df['region']=='northeast' ]['charges']  \n",
    "t, p_value = f_oneway(region_se,region_sw,region_nw,region_ne)\n",
    "print(\"tstat = \",t, \", p_value = \", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since p-value(0.07) > 0.05 (alpha), we failed to reject the null hypothesis and conclude that there is no a difference in the two means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treat_out_cols=df.skew()[df.skew()>1].index.tolist()\n",
    "print(treat_out_cols)\n",
    "df11 = treat_outliers_all(df, treat_out_cols,whisk=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df22=df11.astype({'children':'category'})\n",
    "df22['bmi']=np.log(df.bmi+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.get_dummies(df22,drop_first=True)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df1.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "X=df2.drop('charges',axis=1)\n",
    "y=df2['charges']\n",
    "\n",
    "scale=MinMaxScaler()\n",
    "X=pd.DataFrame(scale.fit_transform(X),columns=X.columns.tolist())\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.3,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinary Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sm.add_constant(X_train)\n",
    "X_test = sm.add_constant(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{X_train.shape},{X_test.shape}')\n",
    "\n",
    "print(f'{y_train.shape},{y_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols=sm.OLS(y_train,X_train)\n",
    "olsfit=ols.fit()\n",
    "olsfit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear model assumptions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collinearity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# we will define a function to check VIF\n",
    "def checking_vif(predictors):\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"feature\"] = predictors.columns\n",
    "\n",
    "    # calculating VIF for each feature\n",
    "    vif[\"VIF\"] = [\n",
    "        variance_inflation_factor(predictors.values, i)\n",
    "        for i in range(len(predictors.columns))\n",
    "    ]\n",
    "    return vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "checking_vif(X_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no VIF values higher than 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# initial list of columns\n",
    "cols = X_train.columns.tolist()   \n",
    "\n",
    "# setting an initial max p-value\n",
    "max_p_value = 1\n",
    "\n",
    "while len(cols) > 0:\n",
    "    # defining the train set\n",
    "    x_train_aux = X_train[cols]   \n",
    "\n",
    "    # fitting the model\n",
    "    model = sm.OLS(y_train, x_train_aux).fit()\n",
    "\n",
    "    # getting the p-values and the maximum p-value\n",
    "    p_values = model.pvalues.drop('const')\n",
    "    max_p_value = max(p_values)\n",
    "\n",
    "    # name of the variable with maximum p-value\n",
    "    feature_with_p_max = p_values.idxmax()\n",
    "    #print(feature_with_p_max) \n",
    "    if max_p_value > 0.05:\n",
    "\n",
    "        cols.remove(feature_with_p_max)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "selected_features = cols\n",
    "\n",
    "\n",
    "print(selected_features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = X_train[selected_features]\n",
    "\n",
    "X_test2 = X_test[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olsmod = sm.OLS(y_train, X_train2)\n",
    "olsres = olsmod.fit()\n",
    "olsres.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred=pd.DataFrame({'fitted':olsres.fittedvalues,'actual':y_train,'res':olsres.resid})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance_regression(olsres,X_train2,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance_regression(olsres,X_test2,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test homoscedasticity  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.stats.api as sms\n",
    "from statsmodels.compat import lzip\n",
    "name = [\"F statistic\", \"p-value\"]\n",
    "test = sms.het_goldfeldquandt(olsres.resid,olsres.model.exog) ## Complete the code to apply the Goldfeldquandt test\n",
    "lzip(name, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.residplot(\n",
    "    data=df_pred, x=\"fitted\", y=\"res\", color=\"purple\", lowess=True\n",
    ")\n",
    "plt.xlabel(\"Fitted Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Fitted vs Residual plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "TEST FOR NORMALITY\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sns.histplot(data=df_pred,x='res') ## Complete the code to plot the distribution of residuals\n",
    "plt.title(\"Normality of residuals\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab\n",
    "import scipy.stats as stats\n",
    "\n",
    "stats.probplot(df_pred.res, dist=\"norm\", plot=pylab) ## Complete the code check Q-Q plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.shapiro(df_pred.res.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assumptions of linear regression are not valied. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(1, 10, num = 10)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv =6, \n",
    "                               verbose=2, random_state=42, n_jobs = -1)# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance_regression( rf_random.best_estimator_,X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance_regression(rf_random.best_estimator_,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg=rf_random.best_estimator_.fit(X_train,y_train).feature_importances_\n",
    "\n",
    "gg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance=pd.DataFrame(gg,index=X_train.columns.tolist(),columns=['coef'])\n",
    "\n",
    "feature_importance.sort_values(by='coef',ascending=False).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_3=rf_random.best_estimator_.fit(X_train[['age','bmi','smoker_yes']],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_3.score(X_train[['age','bmi','smoker_yes']],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_3.score(X_test[['age','bmi','smoker_yes']],y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
